---
title: "Survey sampling basics"
author: Jure Demšar and Erik Štrumbelj, University of Ljubljana
output:
    prettydoc::html_pretty:
      highlight: github
      theme: architect
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
```

# Summary

In this chapter we focus on the basics of sampling in the context of survey sampling. We cover three of the most common probability sampling approaches: simple random sampling, stratified sampling, and cluster sampling. We also briefly discuss non-probability sampling methods: convenience sampling, judgment sampling, quota sampling, and snowball sampling.

# Introduction

Sampling consists of selecting a subset of units from our study population (or data generating process) for the purpose of generalizing the results obtained on this subset to the entire study population. 

Even those of us not familiar with survey sampling are likely to have encountered sampling in randomized algorithms, for example, random forests, or resampling methods, such as cross-validation and bootstrap. The underlying principles are the same as in survey sampling, as is the main reason for applying sampling - sampling is typically used as a means to reduce the resources required for the enquiry. In random forests and bootstrap the resources we are trying to reduce are computational, but survey sampling is more complex, because time, money, human, and other resources have to be considered.

Sampling methods can be divided into two fundamentally different groups: **probability sampling** and **non-probability sampling**. In probability sampling every unit population has a non-zero probability of being selected and that probability is known. Non-probability sampling approaches violate at least one of those two criteria, some units can't be selected or are selected in a way such that the probability of their selection is not known. Simple random sampling is an example of probability sampling, while convenience sampling - for example, selecting units that are close at hand - is an example of non-probability sampling.

The main characteristic of these two groups of approaches to sampling is that in probability sampling we can, using the laws of probability, derive the uncertainty in our results. In other words, we can quantify how representative the sample is of our population. In non-probability sampling, however, that is not possible, at least not without making additional assumptions.

The key component of sampling is the *sampling frame*. The sampling frame is a list of all units in the study population. In other words, it defines a set of units from which a researcher can select a sample of the study population. Without a sampling frame, probability sampling is not possible. However, in cases where our study population has a hierarchical structure, we can avoid the need for a detailed sampling frame for parts of the study population that were not selected at the higher level. We will discuss this in more detail in the Cluster sampling scheme.

In practice, if resources permit, probability sampling is always preferred over non-probability sampling. However, even when in situations where we can use probability sampling, the application is rarely ideal. Two main issues we face are coverage and non-response:

* **Coverage:** Ideally, our sampling frame would cover the entire study population. However, in practice, the sampling frame often does not include all units of the study population (under-coverage) or it includes some units that are not in the study population (over-coverage). For example, if we survey University of Ljubljana (UL) students, we might, if we are not careful, include non-UL students that are only spending a semester here (over-coverage), but miss UL students that are studying abroad this semester (under-coverage).

* **Non-response:** In practice, not all units can be measured. In particular, when dealing with human participants, not all participants will respond. Non-response is essentially a missing values problem.

Both coverage and non-response introduce bias into our results. In general, dealing with these issues requires careful consideration and the gathering of additional information. We will discuss non-response bias in more detail in a later chapter. In this chapter we will, unless noted otherwise, assume that there are no coverage or non-response issues. In other words, the only source of uncertainty will be the sampling itself.

# Probability sampling schemes

Probability sampling approaches can be further subdivided into fixed-size/random-sized sample approaches and equal-probability/unequal-probability approaches. Equal-probability approaches are those where every unit has the same probability of being selected (simple random sampling) as opposed to having different inclusion probabilities (stratified sampling, cluster sampling). A fixed-size sample approach is, as the name suggests, one where the sample size is fixed and known. In this chapter we will only discuss fixed-size sample approaches. A prototypical random-sized sample approach is Poisson sampling, where each unit has a non-zero and potentially different probability of being selected. Poisson sampling is not common in practice, with the exception of surveys where there is a need to coordinate between samples - for example, where we want less or more overlap between two samples drawn from the same population. Poisson sampling is also introduced indirectly by non-response.

In all of our analyses we will focus on estimating the mean. The basic principles are the same for other quantities of interest, but each requires a different model.

## The data

The dataset that we will use throughout is an export of the football player database from the popular computer game Football Manager, we obtained the dataset from [Kaggle](https://www.kaggle.com/ajinkyablaze/football-manager-data).

```{r include=FALSE}
n  <- 200  # sample size
m  <- 100 # number of simulations
df <- readRDS("./data/FM2017.rds")
df <- df[complete.cases(df),]
df$BestPosition <- names(df)[75:89][apply(df[,75:89],1, function(x){which(x == max(x))[1]})]
df <- df[, names(df) %in% c("Name", "NationID", "Age", "Height", "Weight", "BestPosition", "PenaltyTaking")]
df$BestPosition[df$BestPosition == "WingBackLeft"] <- "DefenderLeft"
df$BestPosition[df$BestPosition == "WingBackRight"] <- "DefenderRight"
df$BestPosition[df$BestPosition == "Sweeper"] <- "DefenderCentral"
true_mean <- mean(df$PenaltyTaking)
true_sd   <- sd(df$PenaltyTaking)
```

The dataset contains basic player information and ability estimates for `r nrow(df)` footballers. We will use only a subset of the columns and we will focus on estimating the mean of the Penalty Taking ability of footballers (between 1 - worst and 20 - best):

```{r echo = FALSE}
print(head(df, 10))
```

We will pretend that these footballers are our entire study population. Hence, the true mean $\mu_0 =$ `r round(true_mean, 2)` and true standard deviation $\sigma_0 =$ `r round(true_sd, 2)` of Penalty Taking are known. We will try to estimate this true mean via samples of fixed size $n =$ `r n`.

## Simple random sampling

Simple random sampling is a fixed-sized sample probability sampling scheme with equal inclusion probabilities. In other words, we sample $n$ units uniformly at random.

True to its name, this is the simplest probability sampling scheme and when interested in the mean, we can invoke the Central Limit Theorem that says that the sample average will converge almost surely to the true mean `r round(true_sd, 2)` and the error will be approximately normally distributed with standard deviation $\frac{\sigma_0}{\sqrt{n}} =$ `r round(true_sd / sqrt(n), 3)`.

Let us simulate the process of drawing our $n =$ `r n` samples `r m` times to check if that is indeed the case:

```{r echo=FALSE, warning=FALSE, fig.width=2, fig.height=2}
library(ggplot2)

# simple random sampling
mus <- c()
set.seed(0)
for (i in 1:m) {
  idx <- sample(1:nrow(df), n, rep = T)
  mus <- c(mus, mean(df$PenaltyTaking[idx]))
}

ggplot(data.frame(mu = mus), aes(x = mu)) + geom_histogram(bins = 15) + 
  ylab("freq") + xlab("estimate")
```

The estimate of course varies from sample to sample. The average over all estimates is `r round(mean(mus), 2)` and their standard deviation (hence, standard error of the mean estimates) is `r round(sd(mus), 3)`. Both of these values are within margin of error of what we predicted from theory.

Simple random sampling is indeed simple but also very effective. The main reason why we would not use simple random sampling is that we do not have a sampling frame or we cannot execute it due to resource constraints. Also, if we have additional information about our units, other sampling schemes, such as stratified sampling, will be more effective.

### Bayesian estimation

Because of the CLT, we can, when the sample was obtained via simple random sampling, estimate the mean using a simple normal-likelihood model, even though the data might not be normally distributed. Here is the model in Stan:

```{r comment='', echo = F}
cat(readLines("./models/simple.stan"), sep = '\n')
```

Note that results obtained with this approach will be for all but the smallest sample sizes very similar to those obtained with bootstrapping or a simple normal asymptotic argument of taking the average and quantifying uncertainty with sample standard deviation divided by the squared root of the sample size.

```{r echo=FALSE, warning=FALSE, message=FALSE, results = 'hide'}
library(cmdstanr)
library(posterior)

set.seed(0)
model1 <- cmdstan_model("./models/simple.stan")

if (file.exists("./saved_results/001.rds")) {
  res001 <- readRDS("./saved_results/001.rds")
} else {
  res001 <- NULL
  for (i in 1:50) {
    idx <- sample(1:nrow(df), n, rep = T)
    stan_data <- list(y = df$PenaltyTaking[idx], n = n)
    fit_base <- model1$sample(data = stan_data, chains = 1, iter_sampling = 1000, iter_warmup = 200)
    mus <- as_draws_matrix(fit_base$draws("mu"))
    res001 <- rbind(res001, data.frame(mu = mean(mus[,1]), sd = sd(mus[,1])))
  }  
  saveRDS(res001, file = "./saved_results/001.rds")
}

```

We simulated 50 different samples and fit the model separately to each. The posterior mean of course varies from sample to sample. The average over all estimates is `r round(mean(res001$mu), 2)` and their standard deviation across simulations is `r round(sd(res001$mu), 3)`. Both of these are in line with theory.

In practice we typically aren't able to simulate many samples. Instead, we one sample of size $n$ from our population. So we have to rely on t. Below is a histogram of the posterior means for standard deviation for our 50 simulations. We can see how in practice we will sometimes underestimate, sometimes underestimate the variance (why?), but on average, it will be close to the true variance. Of course, the problem gets worse with smaller sample sizes and better with larger sample sizes.

```{r echo=FALSE, warning=FALSE, fig.width=2, fig.height=2}
ggplot(res001, aes(x = sd)) + geom_histogram(bins = 15) + 
  ylab("freq") + xlab("estimate")
```

## Stratified sampling

Stratification is a very simple idea that we might have already encountered in approaches such as stratified cross-validation, which is an example of stratified sampling. We partition the population into non-overlapping partitions called strata. We proceed by probability sampling each stratum independently of other strata. The final sample is a union of all samples. Note that any sampling scheme can be used to sample from each stratum, even another level of stratified sampling, but we will assume that simple random sampling is used.

When using stratified sampling, we have to allocate our total number of samples to individual strata. We will discuss two popular approaches: *proportional allocation* and *optimal allocation*.

Let $k$ be the number of strata and $N_i$, $n_i$ be the size of and the number of samples allocated to the $i-$th strata, respectively. Let $w_i = \frac{n_i}{n}$ be the weights of allocation. We must have $\sum_{i=1}^k n_i = n$, which implies that the weights sum to 1.

### Proportional allocation

In *proportional allocation* we assign $n_i$ so that $w_i = \frac{N_i}{\sum_{i=1}^k N_i}$. That is, that each strata is allocated samples proportional to its size. We can show that proportional allocation stratified sampling will be better than simple random sampling in terms of error when the quantity of interest is very homogeneous within each strata.

In the case of stratified sampling, the sample average $\overline{x} \approx \mu_o$ is replaced by the weighted average $\overline{x}_\text{strat} = \sum_{i=1}^k w_i\overline{x}_i$, where $\overline{x}_i$ are strata sample averages. We know that the sample average is an unbiased estimator and by linearity of expectation, so is $\overline{x}_\text{strat}$.

The efficiency (variance) of these estimators, however, differs:

$$Var[\overline{x}] - Var[\overline{x}_\text{strat}] =\sum_{i=1}^k w_i (\mu_i - \mu_0)^2,$$

where $\mu$ is the grand mean and $\mu_i$ are the strata means. To obtain this result, we will expand the total variance. In the following, let $N$ be the population size, $N_i$ the total number of units in strata $i$, $x_{ij}$ the $j-$th unit in the $i-$th strata, $\mu$ the grand (population) mean, and $\mu_i$ and $\sigma_i^2$ the $i-$th strata's mean and variance.

$$
\sigma^2 = \frac{1}{N}\sum_{i=1}^k \sum_{j = 1}^{N_i} (x_{ij} - \mu)^2 \\
= \frac{1}{N}\sum_{i=1}^k \sum_{j = 1}^{N_i} (x_{ij} - \mu_i + \mu_i - \mu)^2 \\
= \frac{1}{N}\sum_{i=1}^k \sum_{j = 1}^{N_i} (x_{ij} - \mu_i)
^2 + \frac{1}{N}\sum_{i=1}^k \sum_{j = 1}^{N_i} (\mu_i - \mu)^2 \text{ (the third term is 0!)} \\
= \sum_{i=1}^k \frac{N_i}{N}\sigma_i^2 + \sum_{i=1}^k \frac{N_i}{N} (\mu_i - \mu)^2
$$
Ignoring finite population size (or assuming that $n$ is relatively small wrt $N$), we can recognize the idealized $Var[\overline{x}] = \sigma^2$ and  $Var[\overline{x}_\text{strat}] =  \sum_{i=1}^k \frac{N_i}{N}\sigma_i^2$, with $w_i = \frac{N_i}{N}$, which completes the proof.

So, unless all the strata means are exactly the same as the grand mean of the entire population, then the difference between the variances will be positive, otherwise it will be 0. This implies that proportional stratified sampling is expected to be at least as good as simple random sampling regardless of the data or how we stratify them. This also highlights how to stratify optimally - we need strata whose means differ as much as possible from the grand mean. In other words, we should aim for strata with low within-strata variability and high between-strata variability.

### Proportional sampling - simulation

Let us simulate the process of drawing our $n =$ `r n` samples `r m` times to check if that is indeed the case. We believe that Penalty Taking is more in the domain of attacking footballers, so we will stratify based on Best Position:

```{r echo=FALSE}
table(df$BestPosition)
```

Note that in order to do this, we need information about the size of each strata, in our case the number of players by type. This information might not always be available!

```{r echo=FALSE, warning=FALSE, fig.width=2, fig.height=2}
weights <- round(table(df$BestPosition) / nrow(df) * n)
weights[12] <- weights[12] + 2 # a bit of a hack to get exact n

mus002 <- c()
set.seed(0)


if (file.exists("./saved_results/002.rds")) {
  mus002 <- readRDS("./saved_results/002.rds")
} else {
  for (i in 1:m) {
    w_mus <- 0
    for (j in 1:length(weights)) {
      tmp <- df$PenaltyTaking[df$BestPosition == names(weights)[j]]
      idx <- sample(1:length(tmp), weights[j], rep = T)
      w_mus <- w_mus + weights[j] / n * mean(tmp[idx])
    }
    mus002 <- c(mus002, w_mus)
  }
  saveRDS(mus002, "./saved_results/002.rds")
}

ggplot(data.frame(mu = mus002), aes(x = mu)) + geom_histogram(bins = 30) + 
  ylab("freq") + xlab("estimate")

```

The average over all estimates is `r round(mean(mus002), 2)` and their standard deviation is `r round(sd(mus002), 3)`. This is better than simple random sampling.

### Optimal allocation

*Optimal allocation*, also known as Neyman allocation, is the solution to the optimization problem of finding $n_i$ that minimize the variance of the estimator $\overline{x}_\text{strat}$ estimator:

$$w_i = \frac{n_i}{n} = \frac{N_i\sigma_i}{\sum_{i=1}^k N_i\sigma_i}.$$

We can see that the optimal allocation takes into account strata size and strata homogeneity. The larger the strata and the less homogeneous it is, the more samples we allocate to it. The more similar the strata are to the population in terms of variance, the smaller the difference between proportional and optimal allocation:

$$Var[\overline{x}_\text{strat}] - Var[\overline{x}_\text{opt}] = \frac{1}{n}\sum_{i=1}^k w_i (\sigma_i - \sigma_0)^2,$$

where $\sigma_0$ is the grand standard deviation and $\sigma_i$ are the strata standard deviations. We state this without proof.

Even though optimal location is at least as good as proportional allocation in theory, we might still prefer proportional allocation in practice. Optimal allocation requires a good estimate of the within strata variability $\sigma_i$ which is often unknown, although it can be estimated by preliminary sampling. Also, if we are estimating more than one quantity, the $\sigma_i$ will be different for each quantity and it will be impossible to find an allocation that is optimal for all quantities.

### Optimal allocation - simulation

Let us simulate the process of drawing our $n =$ `r n` samples `r m` times, this time for optimal weights:

```{r echo=FALSE, warning=FALSE, fig.width=2, fig.height=2}
# stratified sampling - optimal
sds <- tapply(df$PenaltyTaking, df$BestPosition, sd)
weights_opt <- round(n * weights * sds / sum(weights * sds))
weights_opt[12] <- weights_opt[12] - 2 # a bit of a hack to get exact n

if (file.exists("./saved_results/003.rds")) {
  mus003 <- readRDS("./saved_results/003.rds")
} else {
  mus003 <- c()
  for (i in 1:m) {
    w_mus <- 0
    for (j in 1:length(weights_opt)) {
      tmp <- df$PenaltyTaking[df$BestPosition == names(weights_opt)[j]]
      idx <- sample(1:length(tmp), weights_opt[j], rep = T)
      w_mus <- w_mus + weights[j] / n * mean(tmp[idx])
    }
    mus003 <- c(mus003, w_mus)
  }
    saveRDS(mus003, "./saved_results/003.rds")
}

ggplot(data.frame(mu = mus003), aes(x = mu)) + geom_histogram(bins = 30) + 
  ylab("freq") + xlab("estimate")
```

The average over all estimates is `r round(mean(mus003), 2)` and their standard deviation is `r round(sd(mus003), 3)`. This is better than simple random sampling and proportional stratified sampling.

## Bayesian estimation for stratified sampling

A simple but effective way of modeling data that arise from stratified sampling is to model each strata separately according to the sampling procedure used for that strata (in our case, a simple random sample). The estimate of the grand mean is a weighted sum of the individual means, where the weights are derived from the proportion of each strata in the entire population:

```{r comment='', echo = F}
cat(readLines("./models/stratified.stan"), sep = '\n')
```

```{r echo=FALSE, warning=FALSE, message=FALSE, results = 'hide'}
model2 <- cmdstan_model("./models/stratified.stan")

weights <- round(table(df$BestPosition) / nrow(df) * n)
weights[12] <- weights[12] + 2 # a bit of a hack to get exact n
set.seed(0)
res <- NULL

if (file.exists("./saved_results/004.rds")) {
  res004 <- readRDS("./saved_results/004.rds")
} else {
  for (i in 1:m) {
    obs <- NULL
      w_mus <- 0
    for (j in 1:length(weights)) {
    
      tmp <- df$PenaltyTaking[df$BestPosition == names(weights)[j]]
      idx <- sample(1:length(tmp), weights[j], rep = T)
      obs <- rbind(obs, data.frame(ID = j, y = tmp[idx]))
      w_mus <- w_mus + weights[j] / n * mean(tmp[idx])
    }
    
    stan_data <- list(y = obs$y, id = obs$ID, n = n, k = length(weights), w = as.numeric(weights) / n)
    fit_base <- model2$sample(data = stan_data, chains = 1, iter_sampling = 1000, iter_warmup = 200)
    mus <- as_draws_matrix(fit_base$draws("mu_est"))
    res004 <- rbind(res004, data.frame(mu = mean(mus),
                                 med = median(mus),
                                 sd = sd(mus), 
                                 w_mus = w_mus,
                                 mu_obs = mean(obs$y)))
  
  }
  
  saveRDS(res004, file = "./saved_results/004.rds")
}

print(summary(res004))

```

## Cluster sampling

The cluster sampling scheme is similar to the stratified sampling scheme in that we partition the data into non-overlapping partitions, this time called clusters. However, instead of sampling from all clusters, like we do in stratified sampling, we select at random only a subset of the clusters and then measure all units in the selected clusters. This is called one-stage clustering. Instead of measuring all units in a cluster, we can again estimate the quantity of interest for a cluster using only simple random sample from that cluster. This is called two-stage clustering. The approach can be extend to an arbitrary number of levels and it is called multi-stage clustering.

Cluster sampling is an alternative to simple random sampling and stratified sampling that is used when it is difficult to construct a sampling frame for the entire study population. For example, it would be difficult to get a list of all University of Ljubljana students, but it is relatively easy to get a list of all Faculties and withing each Faculty the list of all study programs. Once a study program is selected, it is easier to establish contact with all students from that program. Cluster sampling is also used to reduce the cost of sampling. For example, if our study involves units that are geographically spread out, it might be cheaper to first select at random only a few geographical locations and then sample from those locations.

Unfortunately, cluster sampling comes at a cost in efficiency. In stratified sampling we benefit from having strata with means that differ as much as possible. We achieve this by having a lot of between-strata variability but little within-strata variability. This very mechanism that benefits us in stratified sampling makes cluster sampling less effective. If we have very little within-cluster variability and a lot of between-cluster variability that will increase the uncertainty of our estimates. So, in contrast with stratified sampling, we want to have a lot of within-cluster variability and little between-cluster variability.

### Simulation

Let us simulate the process of two-stage cluster sampling our $n =$ `r n` samples `r m` times to check if that is indeed the case. We will sample 5 clusters based on Best Position with `r n / 5` units sampled with simple random sampling from each cluster.

We will estimate the mean and quantify the uncertainty in several different ways.

First, we will estimate the mean as a weighted average of sampled clusters, where each cluster's weight will be proportional to the sum of weights of sampled clusters (not all clusters!).

Second, we will estimate the mean and standard error using the simple asymptotic-normality based approach. That is, we will pretend that we have simple random sampling. In theory, we should underestimate the uncertainty.

Third, we will use a two-level bootstrap. Instead of the most common use of bootstrap, where we resample the samples with replacement, we will first resample the 5 clusters with replacement and then sample with replacement within each bootstrapped cluster. Finally, we will compute the mean estimate as in the first approach, weighted by cluster size. This will give us one bootstrap estimate and we will repeat the process several times, to quantify uncertainty. In essence, we mimic the two-level structure of the data, capturing both the uncertainty due to cluster sampling and the uncertainty due to unit sampling.

```{r}
h_bootstrap <- function(x, group, w, m = 100) {
  k   <- length(w)
  mus <- c()
  for (i in 1:m) {    # bootstrap iteration
    tot_w <- 0
    tot_x <- 0
    for (j in 1:k) {  # resample clusters
      idx <- sample(1:k, 1)
      tmp <- x[group == idx]
      tot_w <- tot_w + w[idx]
      tmp <- sample(tmp, length(tmp), rep = T) # resample withing cluster
      tot_x <- tot_x + mean(tmp) * w[idx]
    }
    mus <- c(mus, tot_x / tot_w)
  }
  return (list(mu = mean(mus), SE = sd(mus)))
}
```

Finally, we will use a Bayesian model that models the two-level strucure. Similarly, we will estimate the mean of each cluster separately and the grand mean by sampling with replacement from the clusters:

```{r comment='', echo = F}
cat(readLines("./models/clustered.stan"), sep = '\n')
```

Note that

Now we are ready to simulate:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="hide", fig.width=2, fig.height=2}
model3 <- cmdstan_model("./models/clustered.stan")

# cluster sampling
weights <- table(df$BestPosition) / nrow(df) 
bp <- unique(df$BestPosition)

if (file.exists("./saved_results/005.rds")) {
  res <- readRDS("./saved_results/005.rds")
} else {
  res <- NULL
  for (i in 1:m) {
    x_total <- 0
    w_total <- 0
    all_y   <- c()
    all_g   <- c()
    all_w   <- c()
    for (j in 1:5) {
      idw <- sample(1:length(bp), 1)
      tmp <- df$PenaltyTaking[df$BestPosition == bp[idw]]
      idx <- sample(1:length(tmp), n / 5, rep = T)
      all_y <- c(all_y, tmp[idx])
      all_g <- c(all_g, rep(j, length(idx)))
      all_w <- c(all_w, weights[idw])
      x_total <- x_total + weights[idw] * mean(tmp[idx])
      w_total <- w_total + weights[idw]
    }
  

    bts <- h_bootstrap(all_y, all_g, all_w)
    
    stan_data <- list(y = all_y, id = all_g, n = length(all_y), k = length(all_w), w = all_w / sum(all_w))
    fit_base <- model3$sample(data = stan_data, chains = 1, iter_sampling = 1000, iter_warmup = 200)

    mus <- as_draws_matrix(fit_base$draws("mu_est"))

    res <- rbind(res, data.frame(mu = x_total / w_total,          # weighted estimate
                                 mu_simple = mean(all_y),         # naive estimate
                                 se_simple = sd(all_y) / sqrt(n), # SE of naive estimate
                                 mu_boot = bts$mu,                # bootstrap estimate
                                 se_boot = bts$SE,
                                 mu_bayes = mean(mus),            # Bayesian model estimate
                                 se_bayes = sd(mus)))
  }
  saveRDS(res, file = "./saved_results/005.rds")
}

ggplot(data.frame(mu = res$mu), aes(x = mu)) + geom_histogram(bins = 30) + 
  ylab("freq") + xlab("estimate")

```

The average over all estimates is `r round(mean(res$mu), 2)` and their standard deviation is `r round(sd(res$mu), 3)`. This is worse than simple random sampling.

Looking at the averages across all iterations of the simulation, we can see that the naive simple approach underestimates the standard error by a factor of 2! The bootstrap and the Bayesian approach provide us with a good estimate of uncertainty:

```{r echo=FALSE, message=FALSE, warning=FALSE, results="hide", fig.width=2, fig.height=2}
colMeans(res)
```

Again, the quality of the estimate of standard error also varies. For some unfortunate sample of clusters and units withing those clusters, it can be far away from the true standard error. Here is the distribution of posterior means of standard error for the Bayesian model across all iterations of the simulation:

```{r echo=FALSE, warning=FALSE, fig.width=2, fig.height=2}
ggplot(res, aes(x = se_bayes)) + geom_histogram(bins = 15) + 
  ylab("freq") + xlab("estimate")
```

## A note on sampling without replacement

In all of our reasoning in this chapter we have assumed that we are sampling from an infinite population or, equivalently, sampling from a finite population with replacement. The same principles also apply if the sample size is small relative to the population size.

However, when sampling without replacement from a finite population, increasing the sample size decreases the uncertainty in the estimates not only due to more units being observed, but also due to fewer units remaining unobserved. We can intuit this from the extreme case of observing all units in a population without measurement error - there is no uncertainty in such an estimate!

While classical statistics has well-developed procedures for estimating in a finite population setting, Bayesian statistics does not. The two main reasons are that Bayesian statistics is still young and that developing such Bayesian procedures typically requires the use of relatively complicated models that were until recently very difficult to infer from (2nd generation tools based on HMC, such as Stan, have mitigated this issue). If we find ourselves in a situation where we want to use a Bayesian approach and we have a large sample from a finite population, then this paper is a good starting point:

* Mendoza, M., Contreras-Cristán, A., & Gutiérrez-Peña, E. (2021). Bayesian Analysis of Finite Populations under Simple Random Sampling. Entropy, 23(3), 318.


# Non-probability sampling schemes 

We have already defined non-probability sampling as a sampling approach where some elements of the population have no chance of selection or where we cannot determine the probability of selection. Another way of looking at non-probability sampling is as the sampling approach that we use when no probability sampling approach is feasible. This is typically due to not being able to create a sampling frame. For example, it is impossible to create a sampling frame for all dung beetles and some subpopulations, such as drug additcs are difficult to reach. Instead of random sampling, we select elements based on other criteria and assumptions regarding the study population. This gives rise to exclusion bias, placing limits on the representativeness of the sample. While the results of non-probability sample might be useful in some cases, it is in most cases impossible to make any scientifically sound generalizations to the study population.

## Convenience sampling

Convenience sampling is also known as opportunity sampling or accidental sampling. It is a type of non-probability sampling where the main criterion for selection is that the units are close at hand. An example of convenience sampling would be if we did a statistical enquiry of University of Ljubljana students and only include students from this Bayesian statistics class.

## Judgement sampling

Judgement sampling is also known as purposive sampling or authoritative sampling. It is a type of non-probability sampling where the researchers choose the sample based on their expert knowledge of what would be a representative sample. The criteria we use depend on the context:

* If the goal is to capture the extremes of the variability of some property of our study units, we will select a sample that is diverse with respect to some other variates. That is, samples that are as different from eachother as possible. This is also known as maximum variation sampling. For example, when selecting problems for the course exam, we try to select problems that cover the entire subject matter.

* If the goal is to investigate how a particular property, we will select units that are as similar as possible in all other traits. This is also known as homogeneous sampling. For example, if we are interested in long-term effects of working with a hazardous material, we would select people that are as similar as possible in every other aspect including working with that material for a long time.

* If the goal is to investigate what is typical or atypical, we will select a sample of units that are most or least similar to what is considered typical. This is also known as typical/atypical case sampling. For example, if we are investigating how a typical Faculty of computer information student rates the study program, we would immediately exclude exchange students, female students, students from joint study programs with other faculties, etc.

## Quota sampling

Quota sampling is similar to stratified sampling in that we first partition the study population into non-overlapping partitions based on characteristics we believe are good at explaining the variability in the variates of interest. For each strata we then determine the number of samples we require for that strata - that is, the quota. This might be based on known or approximate proportions of those strata in the study population. The difference between quota and stratified sampling is that in quota sampling we then proceed by meeting the quota of samples by non-probability approaches such as convenience or judgement sampling.

For example, if we were interested in some characteristic of University of Ljubljana students, we might stratify the students based on faculty, gender, and year of study and then try to find at least 3 students from each strata.

## Snowball sampling

Snowball sampling is a non-probability sampling scheme that is based on peoples' social networks. In its most basic form snowball sampling starts with an initial set of eligible subjects and then each subject is asked to suggest all eligible subjects from their social network (friends, family members, acquaintances). This process is stopped when we have a large enough sample or when no new eligible subjects are found.

Snowball sampling has many sources of bias, including the selection of initial set of subjects and biases from how people suggest new eligible subjects. However, in some cases snowball sampling is the only option we have. For example, when dealing with sensitive topics and hidden populations, such as drug addicts, prostitutes, or experts in narrow scientific fields. In such cases there is no readily available sampling frame but if we find a few eligible subjects, it is very likely that they know several other eligible subjects.

Snowball sampling is one of the few non-probability sampling methods for which some rigorous results are available. A variant of snowball sampling called respondent-driven sampling will, under certain assumptions on the properties of the social network and the recruitment process, provide unbiased estimates.

# Recommended readings

* Thompson, S. K. (2012) Sampling, Third Edition. (Chapters 1, 11, 12)

# Additional readings

* Salganik, M. J., & Heckathorn, D. D. (2004). Sampling and estimation in hidden populations using respondent-driven sampling. Sociological methodology, 34(1), 193-240.